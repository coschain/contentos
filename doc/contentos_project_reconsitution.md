## ContentOS简介

ContentOS致力于打造全球范围内最好的内容公链，解决内容产业中利益分配不平均、流量分发不透明、创作产权不易确认、没有完善的信用体系的几个主要问题。构建一个自由、公平的全球内容生态，包含影片、音乐、图文、素材、虚拟礼物、游戏以及工具等，将从移动影音（包含但不仅限于直播、短视频等）细分领域垂直切入，撬动生态。目前合作的全球化影音图文产品有LiveMe、Cheez、PhotoGrid，这三个产品目前已经拥有6000万的月活用户，每天几十万的内容产出。
	
## 关键技术点分析

Steem是基于石墨烯框架进行开发的，石墨烯技术是一种区块链底层技术架构，由Cryptonomex公司开发采用C++语言编写。Cryptonomex的创始人BM（Dan Larimer）也正是Steem的创始人。石墨烯框架是一套区块链工具组，使用DPOS共识机制，包含账号、权限、底层网络通信等组件。ContentOS公链服务是基于Steem代码进行定制化改造和研发，并且加入了对智能合约（WASM）支持，目前已经基本完成了功能开发和代码开源（[项目地址](https://github.com/coschain/contentos)）。在公链开发过程中，结合我们内容生态公链的功能需求，我们对Steem项目做了深入的学习和分析，也发现这套架构存在的一些问题。

### 性能（*TPS*）

在公链的开发过程中，为了更好的了解Steem的*TPS*能力，我们对Steem服务进行写压力测试。

|   分类       |  描述                                            |
| ------------ | ----------------------------------------------- |
|  机器配置     |  AWS EC2（4 CPU、16 GB）                         |
|  客户端       |  Cli_Wallet （改造为异步发送消息机制，提升并发能力）   |
|  服务端       |  官方的 Docker 镜像                                 |
|  版本         |  Stable（19.20）			                         |

##### 测试方式：

一台机器作为种子节点，负责接收wallet发来的请求，并广播给多台witness节点（witness节点只负责产生并广播block，wallet发送的请求并不会直接到达witness，否则会造成witness节点负载不均衡）

##### 测试结果：

###### 1个Witness

| cli_wallet个数   | TPS (估值)        | TPS (峰值)         | TPS (均值)       |       
| --------------- | ----------------- | ----------------- | ---------------- |  
| 1  |121.4478796 | 1874 | 1847.119592 |
| 10 |171.6988218 | 1862 | 1816.724709 |

###### 2个Witness

| cli_wallet个数   | TPS (估值)        | TPS (峰值)         | TPS (均值)       |       
| --------------- | ----------------- | ----------------- | ---------------- |  
|1	 |129.2185809	 |1855	|1812.774851 |
|10	 |190.5568871	 |1856	|1790.230497 |

###### 4个Witness

| cli_wallet个数   | TPS (估值)        | TPS (峰值)         | TPS (均值)       |       
| --------------- | ----------------- | ----------------- | ---------------- |  
|1	 |126.9761778 |1874 |1826.117095 |
|10	 |201.2699487 |1862 |1801.673846 |

###### 21个Witness

| cli_wallet个数   | TPS (估值)        | TPS (峰值)         | TPS (均值)       |       
| --------------- | ----------------- | ----------------- | ---------------- |  
|1  |150.5678872	 |1522	 |1487.716007 |
|10 |286.1352574	 |1770	 |1577.313645 |

> 1、TPS（估值）= 生产节点实际存储的交易数／（最后一个block出块时间 - 第一次收到交易的时间）2、TPS（峰值）= Max{区块i的实际交易数／区块i的实际产生时间}（i 为从收到交易开始到交易处理结束所产生的所有区块号）3、TPS（均值）= 生产节点实际存储的交易数 - 第一个区块交易数 - 最后一个区块的交易数／（出块总时间 - 第一个区块产生时间 - 最后一个区块产生时间）

以上为多轮压力测试得到的平均结果，我们看到TPS最高的是均值，目前只能达到1800~1900 TPS，低于我们对性能要求的预期。TPS是当前所有公链的痛点，为了提升性能，结合我们的公链业务特点，我们考虑先做并行化改造。

Steem对于交易的处理是串行的。这样的设计逻辑简单直接，但缺点也很明显：第一，串行化对相互影响的交易才是必须的，对实际占比很大的独立交易来说，串行的效率不如并行。第二，数据库I/O是交易处理过程的瓶颈，在I/O等待期间CPU空闲，造成算力浪费。Steem的chainbase选用本地内存型数据库，就是为了尽量削弱瓶颈。当我们为了海量数据的业务需求必须放弃chainbase时，容量和扩展性提升的代价是数据库I/O瓶颈效应更加明显。

针对上述串行交易处理的问题，我们提出了并行流水线交易处理架构，通过最大化地利用计算力资源，达到提升TPS的目标。设计思路大致总结为以下几个方面：

- 对状态数据进行高度抽象，打破chainbase存储对象的逻辑，认为存储数据的最小单元为“属性”，逻辑上的数据对象只是“属性”的组合。“属性”之间是独立的，不同“属性”可以安全的并行读写。数据的属性化重构，是并行化的基础。
- 将交易处理过程拆分为4个阶段：解码、读取、执行、回写。解码负责分析出输入、输出属性集和一个结构化的计算函数。读取负责从数据库读出输入属性的值。执行负责根据输入属性运行计算函数，得到输出属性的值。回写负责更新输出属性到数据库。在此基础上，可以构造一个4段并行流水线。只有读取和回写线有数据库IO，它们阻塞时其他线仍在工作，降低计算力的浪费。 
- 流水线控制逻辑，根据交易解码结果，分析交易依赖关系，进行正确的并行调度。

### 数据库存储

现有架构下使用的是chainbase内存数据库作为服务的核心存储，chainbase是符合区块链应用需求的一个事务型数据库。Steem中的chainbase是fork自 GolosChain/chainbase项目（[项目地址](https://github.com/cryptonomex/graphene)）， GolosChain项目的创始人也是BM。chainbase特性是支持多表多索引、状态持久化，支持多进程、嵌套式写事务，支持回滚，项目方宣称性能可以达到100万读写/每秒。chainbase开发语言为C++，使用`boost::interprocess::allocator`实现的依赖mmap文件实现的持久化内存分配作为其数据容器的内存来源，然后使用`boost::interprocess`中提供的多种数据容器来实现数据表。使用起来就像直接操作内存中的数据容器一样，只不过这些内存会通过mmap的形式持久化到文件中去。

在我们实际项目开发的过程中，总结了chainbase的几个问题：

- 缺乏并发控制：其内部没有并发控制，需要用户在外部自己维护并发问题，如果跨进程共享，用户需要通过跨进程锁来保护。 
- 持久化数据格式不跨平台：如果将一个机器上的数据库文件拷贝到其他架构的机器上时，可能产生不兼容的问题。 
- 缺乏宕机一致性：其没用采用一般数据库常用的WAL技术，在变更数据的过程中，内存的同步完全依赖操作系统的刷新机制；当数据变更完成，其采用异步同步(async flush)策略，因此在这2步过程中，系统异常宕机，其数据可能产生损坏。 
- 容量和性能有限，不支持扩展：其采用mmap机制实现内存的持久化，因此其存储容量很难超过机器内存的容量。如果超出机器内存容量，会发生频繁的磁盘IO操作，严重影响数据库的读写性能。读写性能也完全取决于服务器CPU和内存的硬件性能，只能做到有限满足而不是无限扩展。详细数据参考我们的压力测试（[测试代码地址](https://github.com/coschain/db_benchmark)）。 
- 无法做到服务高可用：作为一个单机的内存数据库，一旦发生服务器宕机、掉电等意外情况，可能会引发数据丢失、不可用的情况。 

ContentOS致力于打造全球范围内最好的内容公链，作为服务的核心存储，数据存储容量、读写性能、是否支持高可用这几个关键指标对我们是很重要的。

##### 我们查询了Steemit网站和对应Steem公链的一些统计信息（数据截止到2018年10月23日）

| 分类	| 数据 |
| ---	| --- |
|总文章量 | 6438万 |
|日活用户 | 5万左右 |
|日活跃作者 | 1.2万 |
|月活用户 | 9.4万左右 |
|日新增文章 |1.3万 |
|新增评论 | 4.9万 |
|新增点赞 | 58万左右 |
|数据库总数据容量 |208 GB | 
|每日新增数据量 |142 MB（最新统计2018.10.21~2018.11.15期间的数据）|

> 统计数据来源 1. [steemit-statistics](https://steemit.com/statistics/@arcange/steemit-statistics-20181023-en) 2. [steemsql.com](http://steemsql.com/status)

我们现有合作产品是6000万月活，每天50万的内容产出，已有约1.6亿内容。对比Steemit网站9.4万月活，每天1.2w内容产出，已有文章数6438万。按照月活为600倍、内容产出为40倍、已有内容2.5倍进行估算。我们公链上线后，既有存储容量需要520GB，每天新增数据会达到6GB+，每月新增数据180GB+。做为全球范围的内容生态公链，我们合作的产品还会继续增加，面对如此巨大的数据存储量级和月活跃量级，单机内存上限可能成为使用内存数据库的瓶颈。我们查询了主流云厂商AWS和Google Cloud的计算服务最大内存上限分别为1.952TB和3.844TB（[AWS EC2](https://aws.amazon.com/cn/ec2/pricing/on-demand)，[Google Compute Engine](https://cloud.google.com/compute/pricing)）。主流的Linux操作系统RedHat的限制参数，基于X86_64架构下，目前最大支持内存容量为12TB，单进程可以使用的磁盘可以达到128TB （[RedHat Limits](https://access.redhat.com/articles/rhel-limits)）。按照以上的内容数据及增量速度和服务器、操作系统的限制，我们公链数据存储几年内就会达到物理瓶颈，单机的内存数据库已经不适合我们公链对容量存储的要求，磁盘存储在容量上会有更明显的优势，我们调研了主流的磁盘存储，计划按照实施难度分步骤的先进行单机磁盘存储应用改造，再进行分布式K-V存储应用改造。

##### a、单机磁盘存储
我们调研了主流的单机磁盘存储引擎，经过对比，与现有使用场景比较合适的是使用LevelDB、RocksDB这样基于磁盘的K-V存储，我们为此做了chainbase和LevelDB的性能对比测试。

##### initial dataset = 100,000

|             | multi_index/key: int64 | multi_index/key: string16 | leveldb/key: int64 | leveldb/key: string16 |
| ----------- | ---------------------- | ------------------------- | ------------------ | --------------------- |
| INSERT      | 324,780                | 306,654                   | 190,985            | 193,573               |
| QUERY       | 636,537                | 589,622                   | 36,627             | 39,842                |
| UPDATE      | 143,472                | 142,795                   | 32,107             | 29,975                |
| DELETE      | 568,504                | 515,463                   | 228,10             | 223,41                |

##### initial dataset = 1,000,000

|             | multi_index/key: int64 | multi_index/key: string16 | leveldb/key: int64 | leveldb/key: string16 |
| ----------- | ---------------------- | ------------------------- | ------------------ | --------------------- |
| INSERT      | 270,197                | 241,954                   | 189,322            | 174,581               |
| QUERY       | 291,120                | 357,270                   | 33,231             | 34,575                |
| UPDATE      | 112,574                | 130,191                   | 29,455             | 28,918                |
| DELETE      | 267,665                | 338,753                   | 254,647            | 198,649               |

##### initial dataset = 10,000,000

|             | multi_index/key: int64 | multi_index/key: string16 | leveldb/key: int64 | leveldb/key: string16 |
| ----------- | ---------------------- | ------------------------- | ------------------ | --------------------- |
| INSERT      | 183,183                | 184,535                   | 174,794            | 165,809               |
| QUERY       | 279,173                | 222,123                   | 23,347             | 12,996                |
| UPDATE      | 90,285                 | 78,443                    | 22,795             | 17,041                |
| DELETE      | 275,406                | 207,900                   | 248,015            | 237,812               |

> 详细内容参考：[db_benchmark](https://github.com/coschain/db_benchmark)

可以看到在大量数据前提下，两者插入数据的性能差距不大，查询性能有10倍的差距。根据我们现有的测试结果，LevelDB的查询性能可以达到2.3万TPS，如果加入缓存机制，应该在短期内满足我们公链的使用需求。

##### b、分布式K-V存储服务

调研了Tair，TiDB分布式存储，这类存储服务在读写性能和容量上都可以做到很方便的横向扩展，相比单机引擎的区别是需要跨进程访问，并且公链节点服务部署会相对复杂。

以上是我们用什么存储替换现有的chainbase数据库相关方案，但是chainbase作为Steem公链的核心数据库主要有两个功能：数据存储和支持复杂索引排序查询，K-V存储本身没有索引的支持，结合我们公链的适用场景，我们设计了一套索引排序机制：

- 有限数据排序：把key作为索引存储，支持TOP100以内数据排序。
- 核心业务数据排序：通过key的命名规则支持一级、二级索引。这类索引主要支持核心数据排序的查询，不会超过10~20种。
- 定时排序：支持对数据没有实时依赖的业务查询需求，可以提供自定义定时索引生成机制。
- 对外数据查询排序支持：主要支持外部数据查询需求，可能会是一套独立的数据服务。

## 公链的重写

Steem作为一个优秀的区块链项目，很好的支持了Steemit社交媒体网站的运营。不过正如BM离开Steem项目时说的，Steem作为一个社交媒体公链，功能聚焦的非常狭窄，几乎只能用来做社交。ContentOS要做全世界范围内的内容生态需要支持更多类别的内容业务场景，对公链的能力也提出更高的要求，结合以上对关键技术点的分析，我们需要进一步按照上述的方案对公链进行改造。

由于chainbase是Steem的核心组件，在对chainbase的使用上，Steem的架构设计没有做分层处理，导致服务的其他组件都是直接使用chainbase的接口定义，如果直接替换数据库组件几乎是没有可能。Steem基于MultiIndex设计的各种表结构之间的数据依赖很紧密。例如一个发帖操作，需要影响账号、奖励，带宽，目录分类等多张数据表的同时修改。这种数据紧密的、复杂的耦合组织方式会给并行化改造带来很高的复杂度。

为了公链服务可以更好的支持内容生态业务需求和服务架构的后续扩展能力，我们计划用Golang重写公链，Golang作为一门天生支持并发、支持高性能服务端开发的编程语言，更适合替换已有的C/C++项目。按照我们重新设计的服务架构，合理的功能分层和模块插件化的设计思路可以更好的通过并行化计算和存储改造提升整体服务性能。